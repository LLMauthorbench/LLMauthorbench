 # I Know Which LLM Wrote Your Code Last Summer:  LLM generated Code Stylometry for Authorship Attribution

## 1. Description

**LLM-AuthorBench** is a benchmark for authorship attribution of C code generated by large language models (LLMs). As LLM-generated code becomes more common in production and open-source settings, identifying which model produced a given snippet is increasingly important.

This repository provides a dataset of 32,000 compilable C programs generated by eight state-of-the-art LLMs across diverse programming tasks.
The Google Colab code for training and evaluating both traditional ML classifiers and fine-tuned transformer models (e.g., BERT, CodeBERT, Longformer, and LoRA-adapted Qwen2-1.5B) is available in this repository to support further research.

Key results:

**97% accuracy** distinguishing GPT-4.1 vs GPT-4o (DeBERTa-v3, binary classification).

**95% accuracy in 5-way attribution** across Gemini 2.5-Flash, Claude 3.5, GPT-4.1, Llama 3.1, and Deepseek-V3.



 ## 2. Framework


<img width="1636" alt="Framework" src="https://github.com/user-attachments/assets/29be292e-d709-4b1d-9e1c-af88d7e5d489" />



## 3. Dataset

The dataset containing 32,000 C compilable C samples from 8 different LLMs.

**DATASET JSON:** [LLM-AuthorBench.json.zip](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/LLM-AuthorBench.json.zip)

Sample Entry from the Dataset File:

```python
  {
    "model_name": "claude-3.5-haiku",
    "prompt": "Build a C program to find all roots of a quadratic equation ax^2 + bx + c = 0.",
    "c_code": "#include <stdio.h>\n#include <math.h>\n\n// Function to calculate quadratic equation roots\nvoid calculateQuadraticRoots(double a, double b, double c) {\n    // Variable to store discriminant\n    double discriminant;\n    \n    // Check if coefficient 'a' is zero (not a quadratic equation)\n    if (a == 0) {\n        printf(\"Error: Not a quadratic equation. 'a' cannot be zero.\\n\");\n        return;\n    }\n    \n    // Calculate discriminant\n    discriminant = b * b - 4 * a * c;\n    \n    // Check discriminant value to determine root types\n    if (discriminant > 0) {\n        // Two distinct real roots\n        double root1 = (-b + sqrt(discriminant)) / (2 * a);\n        double root2 = (-b - sqrt(discriminant)) / (2 * a);\n        \n        printf(\"Two distinct real roots:\\n\");\n        printf(\"Root 1 = %.2f\\n\", root1);\n        printf(\"Root 2 = %.2f\\n\", root2);\n    }\n    else if (discriminant == 0) {\n        // One real root (repeated)\n        double root = -b / (2 * a);\n        \n        printf(\"One real root (repeated):\\n\");\n        printf(\"Root = %.2f\\n\", root);\n    }\n    else {\n        // Complex roots\n        double realPart = -b / (2 * a);\n        double imaginaryPart = sqrt(-discriminant) / (2 * a);\n        \n        printf(\"Two complex roots:\\n\");\n        printf(\"Root 1 = %.2f + %.2fi\\n\", realPart, imaginaryPart);\n        printf(\"Root 2 = %.2f - %.2fi\\n\", realPart, imaginaryPart);\n    }\n}\n\n// Main function\nint main() {\n    double a, b, c;\n    \n    // Input coefficients from user\n    printf(\"Enter coefficient a: \");\n    scanf(\"%lf\", &a);\n    \n    printf(\"Enter coefficient b: \");\n    scanf(\"%lf\", &b);\n    \n    printf(\"Enter coefficient c: \");\n    scanf(\"%lf\", &c);\n    \n    // Print the quadratic equation\n    printf(\"\\nQuadratic Equation: %.2fx^2 + %.2fx + %.2f = 0\\n\", a, b, c);\n    \n    // Calculate and display roots\n    calculateQuadraticRoots(a, b, c);\n    \n    return 0;\n}",
    "SHA256_checksum": "00021084180e31a7143c0b2365a61f2b4d7c6906ea52a4ef30b47f062f4ef5b3",
    "char_count": 1891,
    "num_lines": 67,
    "nloc": 41,
    "CC": 2.5,
    "token_size": 628
  }
```
 
## 4. Scripts for Reproducing Research

We are releasing all Google Colab code to **support open science and research**. The results can be reproduced and validated using the following baseline scripts.

### 4.1 Dataset creation module

-   **Description:** This Google Colab script uses openrouter.ai to generate C code samples from various large language models (LLMs).
-  **Purpose:** To create a diverse dataset of automatically generated C code for further evaluation and benchmarking.
- **Script:** ⬇️  [1_DATASET_CREATOR_google_colab.ipynb](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/scripts/1_DATASET_CREATOR_google_colab.ipynb)
 
 
 ### 4.2. Validate the compilability of the generated C code

- **Description:** This Google Colab script verifies that all C code entries in the dataset are compilable by using `gcc -c`. It checks each code sample for syntax errors, type errors, and translation unit correctness, but does not perform linking. In other words, the script assesses the compilability of the dataset's C code, ensuring each file is valid C source code, even though external references may remain unresolved.
- **Purpose:** To ensure that all generated C code samples are valid and compilable as C source files, regardless of external references.
- **Script:** ⬇️  [2_CHECK_COMPILABILITY_google_colab.ipynb](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/scripts/2_CHECK_COMPILABILITY_google_colab.ipynb)

The LLM-AuthorBench dataset includes 32,000 compilable C programs. You can download the dataset from the following link:
[Download LLM-AuthorBench.json.zip](https://github.com/LLMauthorbench/LLMauthorbench/raw/main/LLM-AuthorBench.json.zip)

 ### 4.3. Train and Evaluate BERT for LLM Authorship Attribution


- **Description:** This Google Colab script provides an end-to-end pipeline for training a BERT model on the LLM-AuthorBench dataset for authorship attribution tasks. It covers data preprocessing, model training, and evaluation, enabling users to assess the ability of BERT to identify the authorship of generated texts.
- **Purpose:** To benchmark BERT’s performance on LLM authorship attribution, facilitating research into identifying the origins of AI-generated content.
- **Script:** ⬇️  [3_BERT_training-5-class_google_colab.ipynb](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/scripts/3_BERT_training-5-class_google_colab.ipynb)




 ### 4.4 Train and Evaluate Traditional Machine Learning (ML)  Algorithms for LLM Authorship Attribution
 
- **Description:** This Google Colab script demonstrates how to train and evaluate traditional machine learning models—including Logistic Regression, Random Forest, and Support Vector Machines—on the LLM-AuthorBench dataset for authorship attribution. The script covers feature extraction, model training, and performance evaluation, offering a baseline comparison to deep learning approaches.
- **Purpose:**  To establish traditional machine learning baselines for LLM authorship attribution and compare their effectiveness with transformer-based models like BERT.
- **Script:** ⬇️  [4_TRAIN_Machine_learning_google_colab.ipynb](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/scripts/4_TRAIN_Machine_learning_google_colab.ipynb)


 ### 4.5 Train and Evaluate CodeT5-Authorship for LLM Authorship Attribution
- **Description:** T This Google Colab script provides a complete pipeline for training and evaluating the CodeT5-Authorship model on the LLM-AuthorBench dataset. The script utilizes only the encoder layers of the pretrained CodeT5 architecture and includes data preprocessing, model training, and evaluation steps. The custom classification head, implemented in PyTorch, enables the model to assign code samples to their source LLMs.
- **Purpose:** To benchmark the performance of CodeT5-Authorship on LLM authorship attribution, demonstrating the effectiveness of encoder-based architectures for this specialized classification task.
- **Script:** ⬇️ [5_CodeT5_Authorship_training_google_colab.ipynb](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/scripts/5_CodeT5_Authorship_training_google_colab.ipynb)

## 5. Results

### 5.1 Binary Classification (GPT-4o vs GPT-4.1)

The results for binary classification between GPT-4.1 and GPT-4o (same family) for authorship attribution are presented below, ordered by accuracy. The "Comment" column indicates whether comments were removed during training. This highlights that comments can contain valuable information, as removing them consistently decreases performance by approximately 2–3% in every case.


| Model Name    | Type | Acc (%) | Prec (%) | Time   | Comments | Key Parameters          |
| ------------- | ---- | ------- | -------- | ------ | ---- | ----------------------- |
| DeBERTa-V3    | LLM  | 97.00   | 97.00    | 151:46 | ✔️   | Layers: 12, Token: 2048 |
| QWEN2-1.5B    | LLM  | 96.88   | 96.87    | 179:54 | ✔️   | Layers: 32, Token: 2048 |
| DeBERTa-V3    | LLM  | 96.75   | 96.81    | 45:21  | ✔️   | Layers: 12, Token: 1024 |
| DeBERTa-V3    | LLM  | 96.31   | 96.32    | 27:26  | ✔️   | Layers: 12, Token: 512  |
| Longformer    | LLM  | 96.19   | 96.19    | 117:42 | ✔️   | Layers: 12, Token: 2048 |
| ModernBERT\_B | LLM  | 95.94   | 95.95    | 36:04  | ✔️   | Layers: 12, Token: 512  |
| RoBERTa\_L    | LLM  | 95.68   | 95.76    | 87:35  | ✔️   | Layers: 24, Token: 512  |
| codeBERT      | LLM  | 95.31   | 95.43    | 30:21  | ✔️   | Layers: 12, Token: 512  |
| RoBERTa\_B    | LLM  | 94.81   | 94.87    | 30:21  | ✔️   | Layers: 12, Token: 512  |
| BERT\_B       | LLM  | 94.75   | 94.81    | 31:05  | ✔️   | Layers: 12, Token: 512  |
| DistilBERT\_B | LLM  | 93.81   | 93.82    | 19:04  | ✔️   | Layers: 6, Token: 512   |
| codeBERT      | LLM  | 93.68   | 93.75    | 30:21  | ❌    | Layers: 12, Token: 512  |
| XGBoost       | ML   | 92.20   | 92.20    | 5.21   | ✔️   | Estimators: 400         |
| RoBERTa\_B    | LLM  | 92.81   | 92.84    | 30:33  | ❌    | Layers: 12, Token: 512  |
| Random Forest | ML   | 90.40   | 90.40    | 12.33  | ✔️   | Estimators: 400         |
| BERT\_B       | LLM  | 91.62   | 91.69    | 30:24  | ❌    | Layers: 12, Token: 512  |
| DistilBERT\_B | LLM  | 91.00   | 91.09    | 18:19  | ❌    | Layers: 6, Token: 512   |
| XGBoost       | ML   | 89.70   | 89.70    | 5.98   | ❌    | Estimators: 400         |
| SVM (Kernel)  | ML   | 88.90   | 88.90    | 4.32   | ✔️   | Kernel: RBF             |
| Random Forest | ML   | 88.20   | 88.30    | 11.49  | ❌    | Estimators: 400         |
| Bagging (DT)  | ML   | 84.90   | 84.90    | 6.41   | ✔️   | Estimators: 10          |
| Bagging (DT)  | ML   | 84.70   | 84.80    | 5.93   | ❌    | Estimators: 10          |
| SVM (Linear)  | ML   | 86.40   | 86.40    | 0.09   | ✔️   | Max\_iter=2000          |
| SVM (Kernel)  | ML   | 84.20   | 84.30    | 4.98   | ❌    | Kernel: RBF             |
| KNN           | ML   | 83.50   | 83.50    | 0.00   | ✔️   | Neighbors: 5            |
| SVM (Linear)  | ML   | 80.60   | 80.60    | 0.10   | ❌    | Kernel: Linear          |
| KNN           | ML   | 80.30   | 80.40    | 0.00   | ❌    | Neighbors: 5            |
| Decision Tree | ML   | 77.10   | 77.10    | 0.39   | ✔️   | Max Depth: 8            |
| Decision Tree | ML   | 74.20   | 74.30    | 0.33   | ❌    | Max Depth: 8            |

### 5.1 Multi-class Classification (Gemini-2.5 Flash, Claude-3.5 Haiku, GPT-4.1, Llama 3.1, DeepSeek-V3)

The results for 5-class authorship attribution are presented below, ordered by accuracy. 

| Model Name    | Type | Acc (%) | Prec (%) | Time   | Comment | Key Parameters            |
| ------------- | ---- | ------- | -------- | ------ | --- | ------------------------- |
| Longformer    | LLM  | 95.00   | 95.01    | 604:42 | ✔️  | Layers: 12, Token: 2048   |
| DeBERTa-V3    | LLM  | 94.25   | 94.32    | 107:02 | ✔️  | Layers: 12, Token: 512    |
| DeBERTa-V3    | LLM  | 94.15   | 94.28    | 733:32 | ✔️  | Layers: 12, Token: 2048   |
| DeBERTa-V3    | LLM  | 94.02   | 94.13    | 244:32 | ✔️  | Layers: 12, Token: 1024   |
| codeBERT      | LLM  | 93.52   | 93.64    | 80:40  | ✔️  | Layers: 12, Token: 512    |
| RoBERTa\_B    | LLM  | 93.38   | 93.43    | 80:34  | ✔️  | Layers: 12, Token: 512    |
| DistilBERT\_B | LLM  | 93.02   | 93.06    | 54:58  | ✔️  | Layers: 6, Token: 512     |
| BERT\_B       | LLM  | 92.65   | 92.71    | 85:05  | ✔️  | Layers: 12, Token: 2048   |
| QWEN2-1.5B    | LLM  | 91.87   | 91.86    | 454:43 | ✔️  | Layers: 32, Token: 2048   |
| XGBoost       | ML   | 90.80   | 90.80    | 00:57  | ✔️  | Estimators: 400, Depth: 9 |
| Random Forest | ML   | 88.00   | 88.00    | 00:38  | ✔️  | Estimators: 400           |
| BERT\_B       | LLM  | 85.45   | 85.79    | 80:38  | ❌   | Layers: 12, Token: 2048   |
| SVM (Kernel)  | ML   | 81.40   | 81.40    | 00:40  | ✔️  | Kernel: RBF               |
| Bagging (DT)  | ML   | 78.40   | 78.70    | 00:19  | ✔️  | Estimators: 10            |
| SVM (Linear)  | ML   | 74.60   | 73.90    | 00:01  | ✔️  | Max\_iter=2000            |
| KNN           | ML   | 71.40   | 72.70    | 00:00  | ✔️  | Neighbors: 5              |
| Decision Tree | ML   | 58.90   | 61.20    | 00:01  | ✔️  | Max Depth: 8              |

### 5.3 Traditional ML results on Multi-class Classification


Results for 5-Class Authorship Attribution (20.000 samples) Using Machine Learning. In this approach, TF-IDF is applied to the entire C code. Additional features can be incorporated to further improve the results. 

| Model         | Accuracy | Precision | Recall | F1 Score | Time (s) |
| ------------- | -------- | --------- | ------ | -------- | -------- |
| KNN           | 0.714    | 0.727     | 0.714  | 0.715    | 0.00     |
| Random Forest | 0.880    | 0.880     | 0.880  | 0.879    | 38.85    |
| Bagging (DT)  | 0.784    | 0.787     | 0.784  | 0.785    | 19.09    |
| SVM (Linear)  | 0.746    | 0.739     | 0.746  | 0.737    | 1.10     |
| SVM (Kernel)  | 0.814    | 0.814     | 0.814  | 0.813    | 40.19    |
| Decision Tree | 0.589    | 0.612     | 0.589  | 0.592    | 1.28     |
| XGBoost       | 0.908    | 0.908     | 0.908  | 0.907    | 57.65    |

![image](https://github.com/user-attachments/assets/238e9d9a-36de-48af-b8ca-8c2bce4101ec)

![image](https://github.com/user-attachments/assets/5322f248-8af7-4c44-b6ce-06bcc5eddbae)

