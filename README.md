 # I Know Which LLM Wrote Your Code Last Summer:  LLM generated Code Stylometry for Authorship Attribution



## 1. Description

**LLM-AuthorBench** is a benchmark for authorship attribution of C code generated by large language models (LLMs). As LLM-generated code becomes more common in production and open-source settings, identifying which model produced a given snippet is increasingly important.

- This repository provides a dataset called **LLM-AuthorBench**, consisting of 32,000 compilable C programs generated by eight state-of-the-art LLMs across diverse programming tasks.
- We also release the **CodeT5-Authorship** model, adapted from the original CodeT5+ by removing the decoder layer and retaining only the encoder. The encoder output for the first token is fed into a PyTorch-implemented classification head, consisting of two linear layers with GELU activation and a 20% dropout in between. This setup produces a probability vector for code attribution across classes.
- This repository includes Google Colab code for training and evaluating traditional ML classifiers, the CodeT5-Authorship model, and fine-tuned transformer models (e.g., BERT, CodeBERT, Longformer, and LoRA-adapted Qwen2-1.5B) to support further research.


## 2. Cite Our Work

This work has been **accepted** to the **18th ACM Workshop on Artificial Intelligence and Security (AISec ’25)**. If you use this work, please cite:

```bibtex
@inproceedings{10.1145/3733799.3762964,
  author    = {Bisztray, Tamas and Cherif, Bilel and Dubniczky, Richard A.
               and Gruschka, Nils and Borsos, Bertalan and Ferrag, Mohamed Amine
               and Kovacs, Attila and Mavroeidis, Vasileios and Tihanyi, Norbert},
  title     = {I Know Which LLM Wrote Your Code Last Summer:
               LLM Generated Code Stylometry for Authorship Attribution},
  publisher = {Association for Computing Machinery},
  booktitle = {Proceedings of the 18th ACM Workshop on Artificial Intelligence and Security},
  year      = {2026},
  pages     = {28--39},
  series = {AISec '25},
  publisher = {Association for Computing Machinery},
  doi       = {10.1145/3733799.3762964},
  url       = {https://doi.org/10.1145/3733799.3762964}
}
```
 ## 3. Framework
 
The framework for creating the LLM-AuthorBench dataset is illustrated in the following figure.

<img width="1636" alt="Framework" src="https://github.com/user-attachments/assets/29be292e-d709-4b1d-9e1c-af88d7e5d489" />



## 4. Dataset

The dataset containing 32,000 C compilable C samples from 8 different LLMs.
This dataset can serve as a reference point for comparing different models in LLM C code authorship attribution.

**DATASET JSON:** [LLM-AuthorBench.json.zip](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/LLM-AuthorBench.json.zip)

Sample Entry from the Dataset File:

```python
  {
    "model_name": "claude-3.5-haiku",
    "prompt": "Build a C program to find all roots of a quadratic equation ax^2 + bx + c = 0.",
    "c_code": "#include <stdio.h>\n#include <math.h>\n\n// Function to calculate quadratic equation roots\nvoid calculateQuadraticRoots(double a, double b, double c) {\n    // Variable to store discriminant\n    double discriminant;\n    \n    // Check if coefficient 'a' is zero (not a quadratic equation)\n    if (a == 0) {\n        printf(\"Error: Not a quadratic equation. 'a' cannot be zero.\\n\");\n        return;\n    }\n    \n    // Calculate discriminant\n    discriminant = b * b - 4 * a * c;\n    \n    // Check discriminant value to determine root types\n    if (discriminant > 0) {\n        // Two distinct real roots\n        double root1 = (-b + sqrt(discriminant)) / (2 * a);\n        double root2 = (-b - sqrt(discriminant)) / (2 * a);\n        \n        printf(\"Two distinct real roots:\\n\");\n        printf(\"Root 1 = %.2f\\n\", root1);\n        printf(\"Root 2 = %.2f\\n\", root2);\n    }\n    else if (discriminant == 0) {\n        // One real root (repeated)\n        double root = -b / (2 * a);\n        \n        printf(\"One real root (repeated):\\n\");\n        printf(\"Root = %.2f\\n\", root);\n    }\n    else {\n        // Complex roots\n        double realPart = -b / (2 * a);\n        double imaginaryPart = sqrt(-discriminant) / (2 * a);\n        \n        printf(\"Two complex roots:\\n\");\n        printf(\"Root 1 = %.2f + %.2fi\\n\", realPart, imaginaryPart);\n        printf(\"Root 2 = %.2f - %.2fi\\n\", realPart, imaginaryPart);\n    }\n}\n\n// Main function\nint main() {\n    double a, b, c;\n    \n    // Input coefficients from user\n    printf(\"Enter coefficient a: \");\n    scanf(\"%lf\", &a);\n    \n    printf(\"Enter coefficient b: \");\n    scanf(\"%lf\", &b);\n    \n    printf(\"Enter coefficient c: \");\n    scanf(\"%lf\", &c);\n    \n    // Print the quadratic equation\n    printf(\"\\nQuadratic Equation: %.2fx^2 + %.2fx + %.2f = 0\\n\", a, b, c);\n    \n    // Calculate and display roots\n    calculateQuadraticRoots(a, b, c);\n    \n    return 0;\n}",
    "SHA256_checksum": "00021084180e31a7143c0b2365a61f2b4d7c6906ea52a4ef30b47f062f4ef5b3",
    "char_count": 1891,
    "num_lines": 67,
    "nloc": 41,
    "CC": 2.5,
    "token_size": 628
  }
```
 
## 5. Scripts for Reproducing Research

We are releasing all Google Colab code to **support open science and research**. The results can be reproduced and validated using the following baseline scripts.

### 5.1 Dataset creation module

-   **Description:** This Google Colab script uses openrouter.ai to generate C code samples from various large language models (LLMs).
-  **Purpose:** To create a diverse dataset of automatically generated C code for further evaluation and benchmarking.
- **Script:** ⬇️  [1_DATASET_CREATOR_google_colab.ipynb](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/scripts/1_DATASET_CREATOR_google_colab.ipynb)
 
 
 ### 5.2. Validate the compilability of the generated C code

- **Description:** This Google Colab script verifies that all C code entries in the dataset are compilable by using `gcc -c`. It checks each code sample for syntax errors, type errors, and translation unit correctness, but does not perform linking. In other words, the script assesses the compilability of the dataset's C code, ensuring each file is valid C source code, even though external references may remain unresolved.
- **Purpose:** To ensure that all generated C code samples are valid and compilable as C source files, regardless of external references.
- **Script:** ⬇️  [2_CHECK_COMPILABILITY_google_colab.ipynb](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/scripts/2_CHECK_COMPILABILITY_google_colab.ipynb)

The LLM-AuthorBench dataset includes 32,000 compilable C programs. You can download the dataset from the following link:
[Download LLM-AuthorBench.json.zip](https://github.com/LLMauthorbench/LLMauthorbench/raw/main/LLM-AuthorBench.json.zip)

 ### 5.3. Train and Evaluate BERT for LLM Authorship Attribution


- **Description:** This Google Colab script provides an end-to-end pipeline for training a BERT model on the LLM-AuthorBench dataset for authorship attribution tasks. It covers data preprocessing, model training, and evaluation, enabling users to assess the ability of BERT to identify the authorship of generated texts.
- **Purpose:** To benchmark BERT’s performance on LLM authorship attribution, facilitating research into identifying the origins of AI-generated content.
- **Script:** ⬇️  [3_BERT_training-5-class_google_colab.ipynb](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/scripts/3_BERT_training-5-class_google_colab.ipynb)




 ### 5.4 Train and Evaluate Traditional Machine Learning (ML)  Algorithms for LLM Authorship Attribution
 
- **Description:** This Google Colab script demonstrates how to train and evaluate traditional machine learning models—including Logistic Regression, Random Forest, and Support Vector Machines—on the LLM-AuthorBench dataset for authorship attribution. The script covers feature extraction, model training, and performance evaluation, offering a baseline comparison to deep learning approaches.
- **Purpose:**  To establish traditional machine learning baselines for LLM authorship attribution and compare their effectiveness with transformer-based models like BERT.
- **Script:** ⬇️  [4_TRAIN_Machine_learning_google_colab.ipynb](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/scripts/4_TRAIN_Machine_learning_google_colab.ipynb)


 ### 5.5 Train and Evaluate CodeT5-Authorship for LLM Authorship Attribution
 
- **Description:** This Google Colab script provides a complete pipeline for training and evaluating the CodeT5-Authorship model on the LLM-AuthorBench dataset. The script utilizes only the encoder layers of the pretrained CodeT5 architecture and includes data preprocessing, model training, and evaluation steps. The custom classification head, implemented in PyTorch, enables the model to assign code samples to their source LLMs.
- **Purpose:** To benchmark the performance of CodeT5-Authorship on LLM authorship attribution, demonstrating the effectiveness of encoder-based architectures for this specialized classification task.
- **Script:** ⬇️ [5_CodeT5-Authorship_5-class_google_colab.ipynb](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/scripts/5_CodeT5-Authorship_5-class_google_colab.ipynb)



## 6. Results

### 6.1 Binary Classification (GPT-4o vs GPT-4.1)

The results for binary classification between GPT-4.1 and GPT-4o (same family) for authorship attribution are presented below, ordered by accuracy. The "Comment" column indicates whether comments were removed during training. This highlights that comments can contain valuable information, as removing them consistently decreases performance by approximately 2–3% in every case.


| Model Name    | Type | Acc (%) | Prec (%) | Time   | Comments | Key Parameters          |
| ------------- | ---- | ------- | -------- | ------ | ---- | ----------------------- |
| CodeT5-Authorship    | LLM  | 97.56   | 97.59    | 74:14 | ✔️   | Layers: 24, Token: 512 |
| DeBERTa-V3    | LLM  | 97.00   | 97.00    | 151:46 | ✔️   | Layers: 12, Token: 2048 |
| QWEN2-1.5B    | LLM  | 96.88   | 96.87    | 179:54 | ✔️   | Layers: 32, Token: 2048 |
| DeBERTa-V3    | LLM  | 96.75   | 96.81    | 45:21  | ✔️   | Layers: 12, Token: 1024 |
| DeBERTa-V3    | LLM  | 96.31   | 96.32    | 27:26  | ✔️   | Layers: 12, Token: 512  |
| Longformer    | LLM  | 96.19   | 96.19    | 117:42 | ✔️   | Layers: 12, Token: 2048 |
| ModernBERT\_B | LLM  | 95.94   | 95.95    | 36:04  | ✔️   | Layers: 12, Token: 512  |
| RoBERTa\_L    | LLM  | 95.68   | 95.76    | 87:35  | ✔️   | Layers: 24, Token: 512  |
| codeBERT      | LLM  | 95.31   | 95.43    | 30:21  | ✔️   | Layers: 12, Token: 512  |
| RoBERTa\_B    | LLM  | 94.81   | 94.87    | 30:21  | ✔️   | Layers: 12, Token: 512  |
| BERT\_B       | LLM  | 94.75   | 94.81    | 31:05  | ✔️   | Layers: 12, Token: 512  |
| DistilBERT\_B | LLM  | 93.81   | 93.82    | 19:04  | ✔️   | Layers: 6, Token: 512   |
| codeBERT      | LLM  | 93.68   | 93.75    | 30:21  | ❌    | Layers: 12, Token: 512  |
| XGBoost       | ML   | 92.20   | 92.20    | 5.21   | ✔️   | Estimators: 400         |
| RoBERTa\_B    | LLM  | 92.81   | 92.84    | 30:33  | ❌    | Layers: 12, Token: 512  |
| Random Forest | ML   | 90.40   | 90.40    | 12.33  | ✔️   | Estimators: 400         |
| BERT\_B       | LLM  | 91.62   | 91.69    | 30:24  | ❌    | Layers: 12, Token: 512  |
| DistilBERT\_B | LLM  | 91.00   | 91.09    | 18:19  | ❌    | Layers: 6, Token: 512   |
| XGBoost       | ML   | 89.70   | 89.70    | 5.98   | ❌    | Estimators: 400         |
| SVM (Kernel)  | ML   | 88.90   | 88.90    | 4.32   | ✔️   | Kernel: RBF             |
| Random Forest | ML   | 88.20   | 88.30    | 11.49  | ❌    | Estimators: 400         |
| Bagging (DT)  | ML   | 84.90   | 84.90    | 6.41   | ✔️   | Estimators: 10          |
| Bagging (DT)  | ML   | 84.70   | 84.80    | 5.93   | ❌    | Estimators: 10          |
| SVM (Linear)  | ML   | 86.40   | 86.40    | 0.09   | ✔️   | Max\_iter=2000          |
| SVM (Kernel)  | ML   | 84.20   | 84.30    | 4.98   | ❌    | Kernel: RBF             |
| KNN           | ML   | 83.50   | 83.50    | 0.00   | ✔️   | Neighbors: 5            |
| SVM (Linear)  | ML   | 80.60   | 80.60    | 0.10   | ❌    | Kernel: Linear          |
| KNN           | ML   | 80.30   | 80.40    | 0.00   | ❌    | Neighbors: 5            |
| Decision Tree | ML   | 77.10   | 77.10    | 0.39   | ✔️   | Max Depth: 8            |
| Decision Tree | ML   | 74.20   | 74.30    | 0.33   | ❌    | Max Depth: 8            |

### 6.2 Multi-class Classification (Gemini-2.5 Flash, Claude-3.5 Haiku, GPT-4.1, Llama 3.1, DeepSeek-V3)

The results for 5-class authorship attribution are presented below, ordered by accuracy. 

| Model Name    | Type | Acc (%) | Prec (%) | Time   | Comment | Key Parameters            |
| ------------- | ---- | ------- | -------- | ------ | --- | ------------------------- |
| CodeT5-Authorship    | LLM  | 95.40   | 95.41    | 185:55 | ✔️   | Layers: 24, Token: 512 |
| Longformer    | LLM  | 95.00   | 95.01    | 604:42 | ✔️  | Layers: 12, Token: 2048   |
| DeBERTa-V3    | LLM  | 94.25   | 94.32    | 107:02 | ✔️  | Layers: 12, Token: 512    |
| DeBERTa-V3    | LLM  | 94.15   | 94.28    | 733:32 | ✔️  | Layers: 12, Token: 2048   |
| DeBERTa-V3    | LLM  | 94.02   | 94.13    | 244:32 | ✔️  | Layers: 12, Token: 1024   |
| codeBERT      | LLM  | 93.52   | 93.64    | 80:40  | ✔️  | Layers: 12, Token: 512    |
| RoBERTa\_B    | LLM  | 93.38   | 93.43    | 80:34  | ✔️  | Layers: 12, Token: 512    |
| DistilBERT\_B | LLM  | 93.02   | 93.06    | 54:58  | ✔️  | Layers: 6, Token: 512     |
| BERT\_B       | LLM  | 92.65   | 92.71    | 85:05  | ✔️  | Layers: 12, Token: 2048   |
| QWEN2-1.5B    | LLM  | 91.87   | 91.86    | 454:43 | ✔️  | Layers: 32, Token: 2048   |
| XGBoost       | ML   | 90.80   | 90.80    | 00:57  | ✔️  | Estimators: 400, Depth: 9 |
| Random Forest | ML   | 88.00   | 88.00    | 00:38  | ✔️  | Estimators: 400           |
| BERT\_B       | LLM  | 85.45   | 85.79    | 80:38  | ❌   | Layers: 12, Token: 2048   |
| SVM (Kernel)  | ML   | 81.40   | 81.40    | 00:40  | ✔️  | Kernel: RBF               |
| Bagging (DT)  | ML   | 78.40   | 78.70    | 00:19  | ✔️  | Estimators: 10            |
| SVM (Linear)  | ML   | 74.60   | 73.90    | 00:01  | ✔️  | Max\_iter=2000            |
| KNN           | ML   | 71.40   | 72.70    | 00:00  | ✔️  | Neighbors: 5              |
| Decision Tree | ML   | 58.90   | 61.20    | 00:01  | ✔️  | Max Depth: 8              |




### 6.3 Traditional ML results on Multi-class Classification


Results for 5-Class Authorship Attribution (20.000 samples) Using Machine Learning. In this approach, TF-IDF is applied to the entire C code. Additional features can be incorporated to further improve the results. 

| Model         | Accuracy | Precision | Recall | F1 Score | Time (s) |
| ------------- | -------- | --------- | ------ | -------- | -------- |
| KNN           | 0.714    | 0.727     | 0.714  | 0.715    | 0.00     |
| Random Forest | 0.880    | 0.880     | 0.880  | 0.879    | 38.85    |
| Bagging (DT)  | 0.784    | 0.787     | 0.784  | 0.785    | 19.09    |
| SVM (Linear)  | 0.746    | 0.739     | 0.746  | 0.737    | 1.10     |
| SVM (Kernel)  | 0.814    | 0.814     | 0.814  | 0.813    | 40.19    |
| Decision Tree | 0.589    | 0.612     | 0.589  | 0.592    | 1.28     |
| XGBoost       | 0.908    | 0.908     | 0.908  | 0.907    | 57.65    |

![image](https://github.com/user-attachments/assets/238e9d9a-36de-48af-b8ca-8c2bce4101ec)

![image](https://github.com/user-attachments/assets/5322f248-8af7-4c44-b6ce-06bcc5eddbae)

### 6.4 CodeT5-Authorship architecture and confusion matrix for multi-class classification.

![image](https://github.com/user-attachments/assets/f9aa9fa8-54fe-4ea5-91f3-536d7545e455)

CodeT5-Authorship architecture:

![image](https://github.com/user-attachments/assets/cf49224a-341b-4bb5-a3c1-6e4008d80380)

