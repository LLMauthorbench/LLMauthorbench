 # I Know Which LLM Wrote Your Code Last Summer:  LLM generated Code Stylometry for Authorship Attribution

## 1. Description

**LLM-AuthorBench** is a benchmark for authorship attribution of C code generated by large language models (LLMs). As LLM-generated code becomes more common in production and open-source settings, identifying which model produced a given snippet is increasingly important.

This repository provides a dataset of 32,000 compilable C programs generated by eight state-of-the-art LLMs across diverse programming tasks.
The Google Colab code for training and evaluating both traditional ML classifiers and fine-tuned transformer models (e.g., BERT, CodeBERT, Longformer, and LoRA-adapted Qwen2-1.5B) is available in this repository to support further research.

Key results:

**97% accuracy** distinguishing GPT-4.1 vs GPT-4o (DeBERTa-v3, binary classification).

**95% accuracy in 5-way attribution** across Gemini 2.5-Flash, Claude 3.5, GPT-4.1, Llama 3.1, and Deepseek-V3.



 ## 2. Framework


<img width="1636" alt="Framework" src="https://github.com/user-attachments/assets/29be292e-d709-4b1d-9e1c-af88d7e5d489" />



## 3. Dataset

The dataset containing 32,000 C compilable C samples from 8 different LLMs.

**DATASET JSON:** [LLM-AuthorBench.json.zip](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/LLM-AuthorBench.json.zip)

Sample Entry from the Dataset File:

```python
  {
    "model_name": "claude-3.5-haiku",
    "prompt": "Build a C program to find all roots of a quadratic equation ax^2 + bx + c = 0.",
    "c_code": "#include <stdio.h>\n#include <math.h>\n\n// Function to calculate quadratic equation roots\nvoid calculateQuadraticRoots(double a, double b, double c) {\n    // Variable to store discriminant\n    double discriminant;\n    \n    // Check if coefficient 'a' is zero (not a quadratic equation)\n    if (a == 0) {\n        printf(\"Error: Not a quadratic equation. 'a' cannot be zero.\\n\");\n        return;\n    }\n    \n    // Calculate discriminant\n    discriminant = b * b - 4 * a * c;\n    \n    // Check discriminant value to determine root types\n    if (discriminant > 0) {\n        // Two distinct real roots\n        double root1 = (-b + sqrt(discriminant)) / (2 * a);\n        double root2 = (-b - sqrt(discriminant)) / (2 * a);\n        \n        printf(\"Two distinct real roots:\\n\");\n        printf(\"Root 1 = %.2f\\n\", root1);\n        printf(\"Root 2 = %.2f\\n\", root2);\n    }\n    else if (discriminant == 0) {\n        // One real root (repeated)\n        double root = -b / (2 * a);\n        \n        printf(\"One real root (repeated):\\n\");\n        printf(\"Root = %.2f\\n\", root);\n    }\n    else {\n        // Complex roots\n        double realPart = -b / (2 * a);\n        double imaginaryPart = sqrt(-discriminant) / (2 * a);\n        \n        printf(\"Two complex roots:\\n\");\n        printf(\"Root 1 = %.2f + %.2fi\\n\", realPart, imaginaryPart);\n        printf(\"Root 2 = %.2f - %.2fi\\n\", realPart, imaginaryPart);\n    }\n}\n\n// Main function\nint main() {\n    double a, b, c;\n    \n    // Input coefficients from user\n    printf(\"Enter coefficient a: \");\n    scanf(\"%lf\", &a);\n    \n    printf(\"Enter coefficient b: \");\n    scanf(\"%lf\", &b);\n    \n    printf(\"Enter coefficient c: \");\n    scanf(\"%lf\", &c);\n    \n    // Print the quadratic equation\n    printf(\"\\nQuadratic Equation: %.2fx^2 + %.2fx + %.2f = 0\\n\", a, b, c);\n    \n    // Calculate and display roots\n    calculateQuadraticRoots(a, b, c);\n    \n    return 0;\n}",
    "SHA256_checksum": "00021084180e31a7143c0b2365a61f2b4d7c6906ea52a4ef30b47f062f4ef5b3",
    "char_count": 1891,
    "num_lines": 67,
    "nloc": 41,
    "CC": 2.5,
    "token_size": 628
  }
```
 
## 4. Scripts for Reproducing Research

We are releasing all Google Colab code to **support open science and research**. The results can be reproduced and validated using the following baseline scripts.

### 4.1 Dataset creation module

-   **Description:** This Google Colab script uses openrouter.ai to generate C code samples from various large language models (LLMs).
-  **Purpose:** To create a diverse dataset of automatically generated C code for further evaluation and benchmarking.
- **Script:** [1_DATASET_CREATOR_google_colab.ipynb](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/scripts/1_DATASET_CREATOR_google_colab.ipynb)
 
 
 ### 4.2. Validate the compilability of the generated C code

- **Description:** This Google Colab script verifies that all C code entries in the dataset are compilable by using `gcc -c`. It checks each code sample for syntax errors, type errors, and translation unit correctness, but does not perform linking. In other words, the script assesses the compilability of the dataset's C code, ensuring each file is valid C source code, even though external references may remain unresolved.
- **Purpose:** To ensure that all generated C code samples are valid and compilable as C source files, regardless of external references.
- **Script:** [2_CHECK_COMPILABILITY_google_colab.ipynb](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/scripts/2_CHECK_COMPILABILITY_google_colab.ipynb)

The LLM-AuthorBench dataset includes 32,000 compilable C programs. You can download the dataset from the following link:
[⬇️ Download LLM-AuthorBench.json.zip](https://github.com/LLMauthorbench/LLMauthorbench/raw/main/LLM-AuthorBench.json.zip)

 ### 4.3. Train and Evaluate BERT for LLM Authorship Attribution


- **Description:** This Google Colab script provides an end-to-end pipeline for training a BERT model on the LLM-AuthorBench dataset for authorship attribution tasks. It covers data preprocessing, model training, and evaluation, enabling users to assess the ability of BERT to identify the authorship of generated texts.
- **Purpose:** To benchmark BERT’s performance on LLM authorship attribution, facilitating research into identifying the origins of AI-generated content.
- **Script:**  [3_BERT_training-5-class_google_colab.ipynb](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/scripts/3_BERT_training-5-class_google_colab.ipynb)




 ### 4.4 Train and Evaluate Traditional Machine Learning (ML)  Algorithms for LLM Authorship Attribution
 
- **Description:** This Google Colab script demonstrates how to train and evaluate traditional machine learning models—including Logistic Regression, Random Forest, and Support Vector Machines—on the LLM-AuthorBench dataset for authorship attribution. The script covers feature extraction, model training, and performance evaluation, offering a baseline comparison to deep learning approaches.
- **Purpose:**  To establish traditional machine learning baselines for LLM authorship attribution and compare their effectiveness with transformer-based models like BERT.
- **Script:** [4_TRAIN_Machine_learning_google_colab.ipynbb](https://github.com/LLMauthorbench/LLMauthorbench/blob/main/scripts/4_TRAIN_Machine_learning_google_colab.ipynb)

Results for 5-Class Authorship Attribution (20.000 samples) Using Machine Learning. In this approach, TF-IDF is applied to the entire C code. Additional features can be incorporated to further improve the results. 

| Model         | Accuracy | Precision | Recall | F1 Score | Time (s) |
| ------------- | -------- | --------- | ------ | -------- | -------- |
| KNN           | 0.714    | 0.727     | 0.714  | 0.715    | 0.00     |
| Random Forest | 0.880    | 0.880     | 0.880  | 0.879    | 38.85    |
| Bagging (DT)  | 0.784    | 0.787     | 0.784  | 0.785    | 19.09    |
| SVM (Linear)  | 0.746    | 0.739     | 0.746  | 0.737    | 1.10     |
| SVM (Kernel)  | 0.814    | 0.814     | 0.814  | 0.813    | 40.19    |
| Decision Tree | 0.589    | 0.612     | 0.589  | 0.592    | 1.28     |
| XGBoost       | 0.908    | 0.908     | 0.908  | 0.907    | 57.65    |

![image](https://github.com/user-attachments/assets/238e9d9a-36de-48af-b8ca-8c2bce4101ec)

![image](https://github.com/user-attachments/assets/5322f248-8af7-4c44-b6ce-06bcc5eddbae)


